---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%", 
  error = TRUE
)
```

# practicalPSI

<!-- badges: start -->
<!-- badges: end -->

The goal of practicalPSI is to facilitate post-selection inferential methods in R in user-friendly ways. 

## Installation

You can install the development version of practicalPSI like so:

```{r, eval = FALSE}
devtools::install_github("petersonR/practicalPSI")
```

## Example

Let's say you are wanting to predict gas mileage based on all variables in the `mtcars` data set. 

```{r example}
library(practicalPSI)

data("mtcars")

fit_full <- lm(mpg ~ ., data = mtcars)
summary(fit_full)
```

Hmm, $R^2$ is high but nothing is "significant". What is going on? The model is probably over-specified. Let's use `practicalPSI` to narrow in on what we think the most important factors are. The `step_ic` function can do forwards and backwards selection via AIC by default. 

### `step_ic`


```{r AIC std}
X <- model.matrix(fit_full)[,-1]
head(X)

fit_aic <- step_ic(y = mtcars$mpg, x = X, std = TRUE ) # does not work if x is matrix

x <- mtcars[, !names(mtcars) %in% "mpg"]
fit_aic_std <- step_ic(y = mtcars$mpg, x = x, std = TRUE) # std works if x is dataframe 

fit_aic_std 
```


```{r AIC no std}
fit_aic <- step_ic(y = mtcars$mpg, x = X) 
tidy(fit_aic)
```

AIC selects `wt`, `qsec`, and `am`. But where are the p-values?! This is where post-selection inference comes in; p-values that do not adjust for the selective process are not valid (they will be too small!). We refer to this as a "hybrid" method where selection is performed and ordinary least squares theory is used for inference. 

```{r AIC hybrid}
fit_aic_hybrid <- infer(fit_aic, method = "hybrid")
tidy(fit_aic_hybrid)
```

What if we wanted to adjust for the selective process? Here using `selectiveInference`. 

```{r AIC selectiveinference}
fit_aic_SI <- infer(fit_aic, method = "selectiveinf")
fit_aic_SI 
tidy(fit_aic_SI) 
```

#### Bootstrapping 

What about bootstrapping? 

##### Ignored null

[explain what this is]

```{r AIC bootstrap}
set.seed(1)
fit_aic_boot <- infer(fit_aic, method = "boot", B = 100) 
fit_aic_boot
tidy(fit_aic_boot) 
```

##### Confident null non-selections

[Explain what this is]

```{r AIC bootstrap 2}
set.seed(1)
fit_aic_boot <- infer(fit_aic, method = "boot", B = 100, nonselection = "confident_nulls") 
fit_aic_boot
tidy(fit_aic_boot) 
```

##### uncertain null non-selections

[Explain what this is]

```{r}
set.seed(1)
fit_aic_boot <- infer(fit_aic, method = "boot", B = 100, nonselection = "uncertain_nulls") 
fit_aic_boot
tidy(fit_aic_boot) 
```


In either case, we find the selections no longer significant after adjusting for the selective inference process. What gives? 

```{r}
fit_bic <- step_ic(y = mtcars$mpg, x=X, penalty = "BIC")
fit_bic 
tidy(infer(fit_bic)) # should be same as fit_aic_hybrid, same model (I think it shouls be same as fit_bic and it is??)
```

Selective inference: 

```{r}
fit_bic_SI <- infer(fit_bic, method = "selectiveinf")
fit_bic_SI # Much smaller intervals

tidy(fit_bic_SI)
```

What about bootstrapping? 

```{r}
set.seed(1)
fit_bic_boot <- infer(fit_bic, method = "boot", B = 100) 

tidy(fit_bic_boot )
```



### `pen_cv`

We can also try selecting the most important factors are by penalized models. We can fit the lasso with cross validation and selects coefficets assocaited with  `lambda_min` (default) or `lambda.1se`

```{r}
set.seed(12)
fit_lso <- pen_cv(y = mtcars$mpg, x = X) # does not work

fit_lso <- pen_cv(y = mtcars$mpg, x = x) # it works with x as data-frame
fit_lso # nothing selected? 
tidy(fit_lso)

```

As we can see with lasso, we get same three variables at stepwise AIC (`wt`,`qaec`,`am`), but their coefficients are different. Even without adjusting for post-selective inference, lasso  by itself does not provide any p-value or CI to do inference. We can again perfom do hybrid method where selection is performed with LASSO and ordinary least squares theory is used for inference.


```{r}
fit_lasso_hybrid <-infer(fit_lso,method = "hybrid") # does not work
tidy(fit_lasso_hybrid)
```

What if we wanted to adjust for the selective process? Here using `selectiveInference`. 

```{r}
fit_lasso_SI <- infer(fit_lso, method = "selectiveinf")
tidy(fit_lasso_SI)
```

What about bootstrapping?

```{r}
set.seed(12)
fit_lasso_boot <- infer(fit_lso, method = "boot", B = 100) # needs print method
tidy(fit_lasso_boot) # needs tidy method
```





