---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%", 
  error = TRUE
)
```

# practicalPSI

<!-- badges: start -->
<!-- badges: end -->

The goal of practicalPSI is to facilitate post-selection inferential methods in R in user-friendly ways. 

## Installation

You can install the development version of practicalPSI like so:

```{r, eval = FALSE}
devtools::install_github("petersonR/practicalPSI")
```

## Example

Let's say you are wanting to predict gas mileage based on all variables in the `mtcars` data set. 

```{r example}
library(practicalPSI)

data("mtcars")

fit_full <- lm(mpg ~ ., data = mtcars)
summary(fit_full)
```

Hmm, $R^2$ is high but nothing is "significant". What is going on? The model is probably over-specified. Let's use `practicalPSI` to narrow in on what we think the most important factors are. The `step_ic` function can do forwards and backwards selection via AIC by default. 

### `step_ic`


```{r AIC std}
X <- model.matrix(fit_full)[,-1]
head(X)

fit_aic <- step_ic(y = mtcars$mpg, x = X, std = TRUE,direction = "forward" ) # does not work if x is matrix

#x <- mtcars[, !names(mtcars) %in% "mpg"]
#fit_aic_std <- step_ic(y = mtcars$mpg, x = x, std = TRUE) # std works if x is dataframe 

fit_aic
```

`step_ic()` function creates a `selector_ic` class which we can pass to `tidy()` function to get results in tibble:


```{r AIC no std}
fit_aic <- step_ic(y = mtcars$mpg, x = X) 
tidy(fit_aic)
```

AIC selects `wt`, `cyl`, and `hp`. But where are the p-values?! This is where post-selection inference comes in; p-values that do not adjust for the selective process are not valid (they will be too small!). We refer to this as a "hybrid" method where selection is performed and ordinary least squares theory is used for inference. 

```{r AIC hybrid}
fit_aic_hybrid <- infer(fit_aic, method = "hybrid")
tidy(fit_aic_hybrid)
```

What if we wanted to adjust for the selective process? Here we can just change `method` option in `infer()` function to using `selectiveinf` to use  `selectiveInference` method. 

```{r AIC selectiveinference}
fit_aic_SI <- infer(fit_aic, method = "selectiveinf")
fit_aic_SI 
tidy(fit_aic_SI) 
```

#### Bootstrapping 

What about bootstrapping? 

When bootstrapping the data and performing model selection, we have to decide how to get distribution of variables when sometimes that variable could be selected while other times, it doens't. 

##### Ignored null

In this case, we first perform the variable selection using whole data set and prefer model selection method using either `step_ic()` or `pen_cv()` functions. Then for inference, we only focus on variables that are selected on whole data when doing bootstrap. 

To obtain the bootstrap distribution of each $\hat{\beta}_j$ selected by model selection procedure,we proceed as follows:  For each bootstrap iteration, we resample the data with replacement, apply the same model selection method to the re sampled data, and save the coefficients for variables that were selected in first step. If a variable in the prime model in step 1  is not selected in the current bootstrap model, we set its coefficient to zero. This process provides a bootstrap distribution for each variable that was selected in full dataset, allowing us to calculate confidence intervals based on the quantiles of these distributions. 

```{r AIC bootstrap}
set.seed(1)
fit_aic_boot <- infer(fit_aic, method = "boot", B = 100) 
fit_aic_boot
tidy(fit_aic_boot) 
```

##### Confident null non-selections

In this setting, we are interested in making inferences for all variables, including those not selected by the prime model using full data(i.g., using `step_ic()` or `pen_cv()` on full dataset). Compared to the previous approach, the primary difference here is that, for each bootstrap iteration, after applying the model same selection method to boostrap sample, we retain the coefficients for all variables, regardless of whether they were included in the prime model or not. For any variable not selected in a given bootstrap sample, its coefficient is set to zero. This process produces a bootstrap distribution for all $p$ variables, allowing us to calculate CIs by using the quantiles of these distributions. 

```{r AIC bootstrap 2}
set.seed(1)
fit_aic_boot <- infer(fit_aic, method = "boot", B = 100, nonselection = "confident_nulls") 
fit_aic_boot
tidy(fit_aic_boot) 
```

##### uncertain null non-selections

[Explain what this is]

```{r}
set.seed(1)
fit_aic_boot <- infer(fit_aic, method = "boot", B = 100, nonselection = "uncertain_nulls") 
fit_aic_boot
tidy(fit_aic_boot) 
```


In either case, we find the selections no longer significant after adjusting for the selective inference process. What gives? 

```{r}
fit_bic <- step_ic(y = mtcars$mpg, x=X, penalty = "BIC")
fit_bic 
```
As, expected with BIC we get fewer varaibles, specicifcially BIC does not select `hp`. And interestingly with hybrid method and ignoring non-selections, we find that both `cyl` and `hp` are signifiacnt as shown below. 

```{r}
tidy(infer(fit_bic)) 
```


Selective inference: 


We see that `Selective inference` gives wider confidence interval and once selection procedure is taken into account, both selected varibales are no longer significant. 


```{r}
fit_bic_SI <- infer(fit_bic, method = "selectiveinf")
fit_bic_SI # Much bigger intervals

tidy(fit_bic_SI)
```

What about bootstrapping? 

```{r}
set.seed(1)
fit_bic_boot <- infer(fit_bic, method = "boot", B = 100) 

tidy(fit_bic_boot )
```

Again, with bootstrap with see that while `cyl` is only selected 42% of the times compared to `wt` which is selected 87% of the time. However based on bootstrapped CI, neither of them are significant. 

### `pen_cv`

We can also try selecting the most important factors are by penalized models. We can fit the lasso with cross validation and selects coefficets assocaited with  `lambda_min` (default) or `lambda.1se`

```{r}
set.seed(12)
fit_lso <- pen_cv(y = mtcars$mpg, x = X) # does not work
fit_lso 
tidy(fit_lso)

```

As we can see with lasso, we get only 1 overlapping variable as stepwise AIC (`wt`), but two other selected variables are different (`qsec` and `am`) . Even without adjusting for post-selective inference, lasso  by itself does not provide any p-value or CI to do inference. We can again perform do hybrid method where selection is performed with LASSO and ordinary least squares theory is used for inference.


```{r}
fit_lasso_hybrid <-infer(fit_lso,method = "hybrid") 
tidy(fit_lasso_hybrid)
```

What if we wanted to adjust for the selective process? Here using `selectiveInference`. 

```{r}
fit_lasso_SI <- infer(fit_lso, method = "selectiveinf")
tidy(fit_lasso_SI)
```

What about bootstrapping?

```{r}
set.seed(12)
fit_lasso_boot <- infer(fit_lso, method = "boot", B = 100) # needs print method
tidy(fit_lasso_boot) # needs tidy method
```





