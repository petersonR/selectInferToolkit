---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%", 
  error = TRUE
)
```

# practicalPSI

<!-- badges: start -->
<!-- badges: end -->


The `practicalPSI` package aims to addresses a critical challenge in modern statistical modeling: **conducting valid inference after performing model selection.** In the era of big data and complex regression problems, researchers often employ model selection procedures like stepwise regression or penalized methods (e.g., lasso) to identify important variables. However, classical statistical inference assumes the model was chosen *a priori*â€”independent of the data. When this assumption is violated, standard statistical tests and confidence intervals can become overly optimistic, leading to unreliable conclusions.`practicalPSI` provides a user-friendly framework in R to facilitate  post-selection inferential methods.


## Installation

You can install the development version of practicalPSI like so:

```{r, eval = FALSE}
devtools::install_github("petersonR/practicalPSI")
```

## Pacakge overview

The `practicalPSI` provides two broad classes of methods for model selection, stepwise regression methods, and penalized regression methods. The package implements stepwise regression (forward/backward/bidirectional) with the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) via `stepAIC` function from `MASS` package. It also performs penalized regression methods including lasso, ridge, elastic net using `ncvreg` or `glmnet` pacakges and easily returns models associated with choice of either $\lambda_{min}$ or $\lambda_{1se}$. $\lambda_{min}$ is the value of the regularization parameter $\lambda$ that minimizes the cross-validation error, offering the best predictive performance without accounting for model complexity. $\lambda_{1se}$ is a more conservative choice, selected as the largest $\lambda$  where the cross-validation error is within one standard error of  $\lambda_{min}$, resulting in a sparser/condensed model.

Main contribution of the package is that it provides easy ways to implement three different post-selection inference methods after perfoming model selection using one of the methods above: 1) hybrid ordinary least squares (no adjustment) 2) bootstrap and, 3) selective inference (only available for lasso and forward stepwise selection via `selectiveInference` package). Finally, for each post-selection inference method, we provide three approaches to handling non-selections: ignoring them, treating them as confident (point-mass) nulls, and treating them as uncertain nulls. 

## Package Overview

`practicalPSI` offers a comprehensive suite of tools, covering both model selection and the crucial post-selection inference steps.

### Model Selection Methods

The package supports two broad classes of popular model selection techniques:

* **Stepwise Regression:** Implement forward, backward, and bidirectional stepwise regression. User can use either the **Akaike Information Criterion (AIC)** or the **Bayesian Information Criterion (BIC)** as selection criteria, leveraging the `stepAIC` function from the `MASS` package.
* **Penalized Regression:** Easily perform lasso, ridge, and elastic net regressions using the powerful `ncvreg` or `glmnet` packages. `practicalPSI` simplifies the process of obtaining models associated with either $\lambda_{min}$ or $\lambda_{1se}$.
    * $\lambda_{min}$ represents the regularization parameter $\lambda$ that minimizes the cross-validation error, aiming for the best predictive performance.
    * $\lambda_{1se}$ is a more conservative choice, selecting the largest $\lambda$ where the cross-validation error is within one standard error of $\lambda_{min}$. This often results in a sparser, more parsimonious model.

### Post-Selection Inference Methods

The core contribution of `practicalPSI` lies in its ability to provide straightforward implementations of three distinct post-selection inference methods, applicable after you've performed model selection using one of the methods above:

1.  **Hybrid Ordinary Least Squares:** A baseline approach that performs no adjustment for the selection process.
2.  **Bootstrap:** A non-parametric bootstrap method that resamples data to account for selection uncertainty.
3.  **Selective Inference:** A theoretically grounded approach, currently available for lasso and forward stepwise selection through integration with the `selectiveInference` package.

### Handling Non-Selected Variables

A unique and important feature of `practicalPSI` is its flexibility in handling non-selected variables (those not included in the final model). For each post-selection inference method, we offer three distinct approaches:

* **Ignoring them:** Treating non-selected variables as irrelevant.
* **Treating them as confident (point-mass) nulls:** Assuming these variables have exactly zero effect.
* **Treating them as uncertain nulls:** Aim to make inference on all variables but without the assumption that non-selections certainly have a null effect 



## Example

Let's say you are wanting to predict gas mileage based on all variables in the `mtcars` data set. 

```{r example}
library(practicalPSI)

data("mtcars")

fit_full <- lm(mpg ~ ., data = mtcars)
summary(fit_full)
```

Hmm, $R^2$ is high but nothing is "significant". What is going on? The model is probably over-specified. Let's use `practicalPSI` to narrow in on what we think the most important factors are. The `step_ic` function can do forwards and backwards selection via AIC by default. 

### `step_ic`


```{r AIC}
X <- model.matrix(fit_full)[,-1]
fit_aic <- step_ic(y = mtcars$mpg, x = X, direction = "forward") 
fit_aic
```

`step_ic()` function creates a `selector_ic` class which we can pass to `tidy()` function to get results in `tibble`:


```{r AIC tidy}
tidy(fit_aic)
```

AIC selects `wt`, `cyl`, and `hp`. But where are the p-values?! This is where post-selection inference comes in; p-values that do not adjust for the selective process are not valid (they will be too small!). We refer to this as a "hybrid" method where selection is performed and ordinary least squares theory is used for inference. 

```{r AIC hybrid}
fit_aic_hybrid <- infer(fit_aic, method = "hybrid")
fit_aic_hybrid
tidy(fit_aic_hybrid)
```

Thus, `cyl` and `wt` are highly significant while `hp` has a p-value of 0.14. We know these are not valid, as we've used the data both to select the model and now are using the same data to perform inference. What if we wanted to adjust our uncertainty quantification for the selective process? 

Here we can just change `method` option in `infer()` function to using `selectiveinf` to use  `selectiveInference` method. 

```{r AIC selectiveinference}
fit_aic_SI <- infer(fit_aic, method = "selectiveinf")
fit_aic_SI 
tidy(fit_aic_SI) 
```

Here we see that after adjusting for the selective process, we are unable to claim significance of any of these effects (and have quite large CIs). 

With the step wise methods, another option available in package when making inference post-selections is bootstrap `r{eval =F, echo=T} infer (fit_aic, method="boot", B=100)`. Lastly, as stated above for each post-selection inference method, we provide three approaches to handling non-selections: `ignored`(ignoring them), `confident_nulls`- treating them as confident (point-mass) nulls, and `uncertain_nulls` treating them as uncertain nulls. For details about each of theses methods see package vignette.


### `step_ic` vs `pen_cv`

We can also try selecting the most important factors are by penalized models. We can fit the lasso with cross validation and selects coefficients associated with `lambda_min` (default) or `lambda.1se`.

```{r}
set.seed(12)
fit_lso <- pen_cv(y = mtcars$mpg, x = X,penalty= "lasso",lambda="lambda.min") 
fit_lso 
tidy(fit_lso)

```

The lasso also selects the same 3 variables.  The lasso alone does not provide any p-value or CI to do inference, but we can again perform a (questionable) hybrid inferential process where ordinary least squares theory is used for inference as though we never used the data for selection.

```{r}
fit_lasso_hybrid <-infer(fit_lso, method = "hybrid") 
tidy(fit_lasso_hybrid)
```

These results indicate that for the model selected by lasso, while `cycl` and `hp` are significant, `wt` is still not significant. Let's see if we can confirm this sentiment after adjusting for the selective process.

*Selective inference* 

```{r}
fit_lasso_SI <- infer(fit_lso, method = "selectiveinf")
tidy(fit_lasso_SI)
```

Here again we see that after adjusting for the selective process, we are only able to claim significance of `wt` but we have larger CIs compared to hybrid approach. 


*Bootstrapping, uncertain null non-selections*

With treating non-selection as uncertain nulls, we're interesting in making inferences for all variables, including those not selected by the prime model using full data (i.e., using `step_ic()` or `pen_cv()` on full data set). To obtain the bootstrap distribution of each $\hat{\beta}_j$ we proceed as follows: For each B iterations, we re sample the data with replacement, apply the same model selection method to the re sampled data, and save the coefficients of selected variables. For any variable not selected in a given bootstrap sample, we regress the model residuals on the each non-selected variables separately to get the beta estimates for non-selections. By doing so, we're trying to explain the residual variance by variables that were not selected by model selection method. This process will produces a bootstrap distribution for all $p$ variables, allowing us to calculate CIs by using the quantiles of these distributions. 


```{r}
set.seed(12)
fit_lasso_boot <- infer(fit_lso, method = "boot", B = 100, 
                        nonselection = "uncertain_nulls") # needs print method
tidy(fit_lasso_boot) # needs tidy method
```

With the `unccertain_nulls` approach, we see that even though `cycl` and `wt` are selected 89% and 98% of the times, non of the variables are significant. 

For a more in depth tutorial and examples for different methods, please consult the package vignette.
