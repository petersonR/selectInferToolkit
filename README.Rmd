---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%", 
  error = TRUE
)
```

# selectInferToolkit

<!-- badges: start -->

<!-- badges: end -->


The `selectInferToolkit` package aims to addresses a critical challenge in modern statistical modeling: **conducting valid inference after performing model selection.** In the era of big data and complex regression problems, researchers often employ model selection procedures like stepwise regression or penalized methods (e.g., lasso) to identify important variables. However, classical statistical inference assumes the model was chosen *a priori*â€”independent of the data. When this assumption is violated, standard statistical tests and confidence intervals can become overly optimistic, leading to unreliable conclusions.`selectInferToolkit` provides a user-friendly framework in R to facilitate  post-selection inferential methods.


## Installation

You can install the development version of selectInferToolkit like so:

```{r, eval = FALSE}
devtools::install_github("petersonR/selectInferToolkit")
```

## Package Overview

`selectInferToolkit` offers a comprehensive suite of tools, covering both model selection and the crucial post-selection inference steps.

### Model Selection Methods

The package supports two broad classes of popular model selection techniques:

* **Stepwise Regression:** Implement forward, backward, and bidirectional stepwise regression. User can use either the **Akaike Information Criterion (AIC)** or the **Bayesian Information Criterion (BIC)** as selection criteria, leveraging the `stepAIC` function from the `MASS` package.
* **Penalized Regression:** Perform lasso, ridge, and elastic net regressions using the powerful `ncvreg` or `glmnet` packages. `selectInferToolkit` simplifies the process of obtaining models associated with either $\lambda_{min}$ or $\lambda_{1se}$.
    * $\lambda_{min}$ represents the regularization parameter $\lambda$ that minimizes the cross-validation error, aiming for the best predictive performance.
    * $\lambda_{1se}$ is a more conservative choice, selecting the largest $\lambda$ where the cross-validation error is within one standard error of $\lambda_{min}$. This often results in a sparser, more parsimonious model.

### Post-Selection Inference Methods

The core contribution of `selectInferToolkit` lies in its ability to provide straightforward implementations of three distinct post-selection inference methods, applicable after you've performed model selection using one of the methods above:

1.  **UPSI**: A baseline approach that performs no adjustment for the selection process. We refer to this as "unadjusted post-selection inference", or "UPSI" for short. Others refer to this as "hybrid-OLS" (in the Gaussian outcome case). 
2.  **Bootstrap:** A non-parametric bootstrap method that resamples data to account for selection uncertainty.
3.  **Selective Inference:** A theoretically grounded approach, currently available for lasso and forward stepwise selection through integration with the `selectiveInference` package.
4. **PIPE**: [currently experimental]

### Handling Non-Selected Variables

`selectInferToolkit` is explicity in its handling of non-selections, which can be set to be:  

* **Ignored:** Treating non-selected variables as irrelevant.
* **Confident (point-mass) nulls:** Assuming these variables have exactly zero effect.
* **Uncertain nulls:** Aim to make inference on all variables but without the assumption that non-selections certainly have a null effect 

### Bootstrapping considerations

In bootstrapping, the above methods of handling nonselections are similar but slightly altered. 

We allow the user to specify their `inference_target` as either `"selections"` or `"all"`, `debias` varies whether the coefficients are re-fit after selection prior to bootstrapped aggregation, and `estimation_data` as either `"in-sample"` or `"out-of-sample"` which facilitates the estimation of the coefficients on the 'left-out' data after the model has been fit within a bootstrapped sample (similar to sample spliting). 

More details on the bootstrapping approach are forthcoming. 

## Example

Let's say you are wanting to predict gas mileage based on all variables in the `mtcars` data set. 

```{r example}
library(selectInferToolkit)
library(dplyr)

data("mtcars")

fit_full <- lm(mpg ~ ., data = mtcars)
summary(fit_full)
```

Hmm, $R^2$ is high but nothing is "significant". What is going on? The model is probably over-specified. Let's use `selectInferToolkit` to narrow in on what we think the most important factors are. The `step_ic` function can do forwards and backwards selection via AIC by default. 

### Stepwise selection


```{r AIC}
fit_aic <- select_stepwise_ic(mpg ~ ., data = mtcars, penalty = "AIC") 
fit_aic
```

Functions prefixed with `select_` create a `selector` class object. We can pass
`selector` objects to `coef`, `predict`, and `tidy()` functions:


```{r AIC tidy}
coef(fit_aic)
tidy(fit_aic)
```

So we find that AIC selects `wt`, `cyl`, and `hp`. But where are the p-values?! 

This is where post-selection inference comes in; p-values that do not adjust for the selective process will be too small. Again, we call this "unadjusted post-selection inference" (or UPSI, for short). 

```{r AIC upsi}
fit_aic_infer_upsi <- infer_upsi(fit_aic, data = mtcars)
fit_aic_infer_upsi
tidy(fit_aic_infer_upsi) %>% dplyr::filter(selected == 1)
```

Thus, `wt` is significant! We know this confidence interval may be biased and too precise, as we've used the data both to select the model and now are using the same data to perform inference (UPSI!). What if we wanted to adjust our uncertainty quantification for the selective process? 

Each selection process has a default `infer` method that is, for the most part, not an UPSI method. The default for stepwise selection via information criteria is bootstrapping. The code below is equivalent to calling `infer_boot`. 

```{r AIC boot}
set.seed(1)
fit_aic_infer_boot <- infer(fit_aic, data = mtcars, B = 100)
fit_aic_infer_boot
tidy(fit_aic_infer_boot) 
```

These results are less promising. Here we can also use a wrapper for  `selectiveInference` method: 

```{r AIC selectiveinference}
fit_aic_SI <- infer_selective(fit_aic, data = mtcars)
fit_aic_SI 
tidy(fit_aic_SI) 
```

Here we see that after adjusting for the selective process, we are unable to claim significance of any of these effects (and have quite large CIs). 

Lastly, as stated above for each post-selection inference method, we provide three approaches to handling non-selections: `ignored` (ignoring them), `confident_nulls` (treating them as confident (point-mass) degenerate nulls at zero), and `uncertain_nulls` (treating them as uncertain). For details about each of theses methods, see the package vignette.

### Penalized regression 

#### Lasso via glmnet 

We can also try selecting the most important factors are by penalized models. We can fit the lasso with cross validation and selects coefficients associated with the `best` predicting model (default, i.e. `lambda.min`) or the most `compact` model that predicts about as well as the best model, i.e. `lambda.1se`.

```{r}
set.seed(12)
fit_lso <- select_glmnet(mpg ~ ., data = mtcars) 
fit_lso 
tidy(fit_lso)

```

The lasso also selects the same 3 variables.  The lasso alone does not provide any p-value or CI to do inference, but UPSIs aside we can again perform a (questionable) hybrid inferential process where ordinary least squares theory is used for inference as though we never used the data for selection.

```{r}
fit_lasso_infer_upsi <-infer_upsi(fit_lso, data = mtcars) 
tidy(fit_lasso_infer_upsi)
```

These results should look familiar: since the model selected by lasso is the same as that selected by stepwise AIC, the UPSI inference method is identical; `wt` is still seemingly significant. 

*Selective inference* 

```{r}
fit_lasso_infer_SI <- infer_selective(fit_lso, data = mtcars)
tidy(fit_lasso_infer_SI)
```

Here we see that after adjusting for the selective process with selective inference, we are able to assert a significant effect of `wt` even after the selective process. The CI for this term is only slightly wide than in the full model. 

*Bootstrapping + debiasing*

If we debias our bootrapped estimates, we no longer are "confident" that a non-selection is akin to a coefficient of zero, thereby treating non-selections as "uncertain nulls". 

This may be the case if we're interesting in making inferences for all variables, including those not selected by the prime model using full data. To obtain the bootstrap distribution of each $\hat{\beta}_j$ we proceed as follows: For each B iterations, we re sample the data with replacement, apply the same model selection method to the re sampled data, and save the coefficients of selected variables. For any variable not selected in a given bootstrap sample, we regress the model residuals on the each non-selected variables separately to get the beta estimates for non-selections. By doing so, we're trying to explain the residual variance by variables that were not selected by model selection method. This process will produces a bootstrap distribution for all $p$ variables, allowing us to calculate CIs by using the quantiles of these distributions. 


```{r}
set.seed(1)
fit_lasso_infer_boot <- infer_boot(fit_lso, data = mtcars, B = 100, inference_target = "all", debias = TRUE) 
tidy(fit_lasso_infer_boot) 
```

With this approach, we see that the `wt` has a CI which does not intersect zero, indicating a significant effect.

Compare these to the results if `debias = FALSE`: 
```{r}
set.seed(1)
fit_lasso_infer_boot <- infer_boot(fit_lso, data = mtcars, B = 100, inference_target = "all", debias = FALSE) 
tidy(fit_lasso_infer_boot) 
```

For a more in depth tutorial and examples for different methods, please consult the package vignette.


#### MCP via ncvreg 

```{r}
set.seed(12)
fit_mcp <- select_ncvreg(mpg ~ ., data = mtcars) 
fit_mcp 
tidy(fit_mcp)

```

```{r}
fit_mcp_infer_upsi <- infer_upsi(fit_mcp, data = mtcars) 
tidy(fit_mcp_infer_upsi)
```

*Bootstrapping*

```{r}
set.seed(12)
fit_mcp_infer_boot <- infer_boot(fit_mcp, data = mtcars, B = 100, inference_target = "all", debias = FALSE) 
tidy(fit_mcp_infer_boot) 
```

```{r}
set.seed(12)
fit_mcp_infer_boot <- infer_boot(fit_mcp, data = mtcars, B = 100, inference_target = "all", debias = TRUE) 
tidy(fit_mcp_infer_boot) 
```

*PIPE*: available for `ncvreg` only

```{r}
fit_mcp_infer_pipe <- infer_pipe(fit_mcp, data = mtcars)
tidy(fit_mcp_infer_pipe)
```

